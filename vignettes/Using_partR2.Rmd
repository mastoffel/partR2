---
title: "Using partR2"
author: "Martin A. Stoffel, Shinichi Nakagawa, Holger Schielzeth"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using_partR2}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


The goal of `partR2` is to partition the varianced explained in generalised linear mixed models (GLMMs) into variation unique to and shared among predictors using semi-partial (or 'part') R^2^, but it also does a few other things. Here is a quick summary of what you can learn about your mixed model with `partR2`:

* Marginal and conditional R^2^ (sensu Nakagawa & Schielzeth, 2013)
* Part R^2^ unique to each predictor and for each combination of predictors (fixed effects)
* Structure coefficients (SC), i.e. the correlation between each predictor and the predicted response (the contribution of a predictor to the model when ignoring all other predictors) (see Yeatts et al., 2017)
* Inclusive R^2^, which we define as explained variance of a predictor in the predicted response irrespective
of covariances with other predictors. It is calculated as SC^2^ * R^2^, the squared structure coefficient
times the R^2^ of the full model.
* Standardised model estimates called beta weights. These are calculated with
beta * (sd(x)/sd(y), where beta is the estimate or slope, x is the predictor and y is the response. 
* Confidence intervals for all relevant estimators through parametric bootstrapping

The workhorse of the package is a single function, `partR2()`. It takes a fitted model from `lme4` (Bates et al. 2015), with either a Gaussian, Poisson or binomial family.

Before we go through some examples, we load the biomass dataset. This is a simulated dataset that aims to mimic a study on biomass production in grasslands. In a nutshell, virtual invertebrated were sampled once every year over 10 successive years from 20 different virtual populations. Temperature and precipitation were measured and overall species diversity and biomass were recorded for each population in each year.

```{r setup, message=FALSE}
# remotes::install_github("mastoffel/partR2)
library(partR2)
library(lme4)
data("biomass")
head(biomass)
```

Before we proceed, we standardise some variables to bring them to the same scale and make their slope estimates comparable (see Schielzeth 2010 for a discussion).
```{r}
biomass[] <- lapply(biomass, function(x) if (is.double(x)) scale(x) else x)
```

## Partitioning R^2^ in a Gaussian mixed model

First of all, we check whether the biomass in our dataset follows a Gaussian distribution.
```{r}
hist(biomass$Biomass, main = "Biomass")
```

That looks alright. Next, we fit a linear mixed model in `lme4`. We assume, that the biomass depends on effects of the year of recording, temperature, precipitation and overall species diversity. We also fit a random effect to account for differences among populations.

```{r, warning=FALSE}
mod1 <- lmer(Biomass ~ Year + Temperature + Precipitation + 
             SpeciesDiversity + (1|Population), 
             data = biomass)
```

Now we would usually do the standard model checks and evaluations to ensure that the model works well. For the sake of simplicity (and because we simulated the data from Gaussian distributions), we skip that step here and go straight into the `partR2` part. First of all, we calculate the overall marginal R^2^ and use parametric bootstrapping to estimate confidence intervals. Marginal R^2^ refers to the variance explained by fixed effect predictors relative to the total variance in the response. Alternatively, we can estimate conditional R^2^ (by setting `R2_type ="conditional"`), which is the variance explained by fixed effects and random effects together relative to the total variance in the response.

Note that we are supplying the fitted `merMod` object (the `lme4` output) and the original dataset (in the `data` argument) used to fit the model.  This is required, because the `merMod` object does not contain all necessary information to perform the `partR2` analysis. There is one important thing to pay attention to: If there are missing observations for some of the predictors/response, `lmer` and `glmer` will remove those rows containing NAs, which will result in a mismatch between the data in the `data` object and the data used fit the model. In order to avoid complications, it is advisable to remove rows with missing data prior to the fitting the model. 

```{r, message=FALSE}
R2_mod1 <- partR2(mod1, R2_type = "marginal", nboot = 10, data = biomass)
R2_mod1
```

The marginal R^2^ is around 60% and confidence intervals are fairly narrow. Temperature and precipitation are highly correlated in the dataset (as they often are in real-life situations) and we want to know how much each of them uniquely explains and what they explain together.

```{r, warning=FALSE}
R2_mod1 <- partR2(mod1, partvars = c("Temperature", "Precipitation"), 
                        R2_type = "marginal", nboot = 10,  data = biomass)
R2_mod1
```

So it seems that temperature and precipitation uniquely only explain around 4% and 9% in the variation in biomass, respectively. Together however, they explain around 39% of the variation! The reason for this is that `partR2` calculates the part R^2^ unique to each predictor by calculating the difference in R^2^ between the full model and a reduced model which does not contain the respective predictor. So when temperature is removed, it's variance explained is largely replaced by precipitation (as both are highly correlated). This is why the R^2^ unique to temperature is only 4%.  

Besides part R^2^s, `partR2` also outputs model estimates and structure coefficients, which are shown when calling the `summary()` function. 

```{r, warning=FALSE}
summary(R2_mod1)
```

The standardised model estimates (called beta weights) show that all four predictors seem to have some effect on biomass because none of the confidence intervals overlaps zero, while the effect of precipitation is the largest. Structure coefficients tell us that both temperature and precipitation are quite strongly correlated with the predicted biomass from the model. Structure coefficients effectively give us the contribution of a predictor to the model prediction in the absence of all other predictors (with a maximum magnitude of ±1). SC are correlation coefficients and by squaring them and multiplying by the marginal R^2^ of the model we get what we call inclusive R^2^ or short IR^2^ (IR^2^ = SC^2^ * R^2^). SC and IR^2^ are large for temperature and precipitation as both are highly correlated to the predicted response, but their semi-partial or part R^2^s (the variance that they uniquely explain) are small due to their correlation. 

## Limiting the amount of combinations

The number of combinations for part R^2^ increases exponantially with 2^n-1 combinations
for n predictors. There are two possibilities to decrease the number of sets for
which R^2^ is calculated and to reduce the computational burden. Our model
`mod1` has four predictors and hence 2^4-1 = 15 combinations. If we only
wanted the part R^2^ for each predictor but not their combinations, we can
specify `max_level`. 

```{r, warning=FALSE}
R2_short <- partR2(mod1, partvars = c("Temperature", "Precipitation", "Year", "SpeciesDiversity"), 
                   R2_type = "marginal", data = biomass, max_level = 1)
R2_short
```

`max_level` = 2 would give all combinations up to a maximum of two predictors 
and so on. 

Another option is `partbatch` which takes a list of character vectors. The
terms in each character vector are then always fitted or removed together. This
is most useful for models with many variables, e.g. when using dummy coding.
When the elements in the list given to partbatch are named, then the output
shows the name of the batch instead of all single list elements.
```{r, warning=FALSE}
R2_batch <- partR2(mod1, partvars = c("SpeciesDiversity"), 
                   partbatch = list(batch1 = c("Temperature", "Precipitation")),
                   R2_type = "marginal", data = biomass)
R2_batch
```


## Plotting

We can now plot the results as forestplots, powered by `ggplot2` (Wickham, 2016). We
added a few arguments to easily customize the plot, see `?forestplot`.

```{r, fig.width = 4, fig.height=2.5}
forestplot(R2_mod1, type = "R2", line_size = 0.5, text_size = 12, point_size = 3)
```

The output of forestplot is a ggplot object so we can change it using ggplot syntax or assemble multiple plots with packages like `patchwork`. We can also plot the model estimates, structure
coefficients or inclusive R^2^s.

```{r, fig.width = 7, fig.height=5}
library(patchwork)
p1 <- forestplot(R2_mod1, type = "R2", text_size = 10)
p2 <- forestplot(R2_mod1, type = "IR2", text_size = 10)
p3 <- forestplot(R2_mod1, type = "SC", text_size = 10)
p4 <- forestplot(R2_mod1, type = "BW", text_size = 10)
(p1 + p2) / (p3 + p4) + plot_annotation(tag_levels = "A")
```

However, if you prepare a publication, you probably want the data to plot it yourself or put it in a table. Here is how the results can be retrieved from a `partR2` object:

```{r}
# An overview
str(R2_mod1, max.level = 1)
```

How to obtain point estimates, confidence intervals and bootstrap replicates.

```{r, results=FALSE}
# point estimates and confidence intervals
R2_mod1$R2   # R2s
R2_mod1$SC   # Structure coefficients
R2_mod1$IR2  # inclusive R2s
R2_mod1$BW # Standardised model estimates
R2_mod1$Ests # Model estimates

# bootstrap replicates
R2_mod1$R2_boot
R2_mod1$SC_boot
R2_mod1$IR2_boot
R2_mod1$BW_boot
R2_mod1$Ests_boot
```

## Check warnings from bootstrapping

Parametric bootstrapping works through simulating new response variables and refitting the model. This sometimes causes warnings when the models based on simulated data are fitted. `partR2` captures those warnings (and messages) and saves them. Lets have a look at potential warnings and messages for the first two simulations.

```{r, warning=FALSE}
R2_mod1$boot_warnings[1:2]
R2_mod1$boot_messages[1:2]
```
No warnings or messages in our case. But you might find something in your own analyses, for example convergence or singularity warnings or messages. Just make sure that you had a look, though these are typically not major problems (at least when the initial model fit wasn't problematic).

## Special cases for part R2s

### Part R^2^s for interactions
There are two ways in which interaction terms can be included in the `formula` argument. The familiar `*` statment allows fitting terms with their main effects and their interactions, while the `:` statement fits only the interactions. This is important to know, because we need to use the `:` version to tell `partR2` that we require a part R^2^ for an interaction.  

It is possible to estimate part R^2^ for models with interaction terms, but requires some thought. To calculate the part R^2^ of a predictor,
the predictor is removed from the model and the part R^2^ is calculated as the difference in R^2^ between the full and the reduced model. When two predictors are also involved in an interaction, we can easily remove the interaction itself. Removing one of the main effects (or predictors) will however not necessarily yield an easily interpretable part R^2^, because main effects and interactions are correlated, and when removing one of them, the other will take over some of the explained variance. Here, we present a few ways on how to deal with interactions:

#### 1) Only estimate R^2^ of the interaction, but not the main effects.

We can fit the model with the `*` statment (which actually fits Temperature + Precipitation + Temperature:Precipitation):

```{r, warning=FALSE}
mod2 <- lmer(Biomass ~ Temperature * Precipitation + (1|Population), 
             data = biomass)
```

For now, we are only interested the explained variation of the interaction itself.
We specify it in the `partvars` argument with `Temperature:Precipitation`.

```{r, warning=FALSE}
R2_mod2a <- partR2(mod2, partvars = c("Temperature:Precipitation"), 
                   data = biomass)
R2_mod2a
```

Here, the part R^2^ of the interaction is relatively small. When a part main effect is also involved in an interaction, the variance explained by the main effect is usually also explained by the interaction, such that the unique variance explained by either the interaction or the main effect is often small.

Next, we'll go through the possibilities to deal with interactions when you are interested in part R^2^ of the main effects and the interaction.

#### 2) Main effects and interactions alone

First of all, we do what's most obvious, we specify both main effects and their interaction in the `partvars` argument. As a consequence, partR2 will iteratively remove each of those predictors and calculate the difference in R^2^ between the full model and each reduced model, respectively, to calculate part R^2^s for each predictor and combination of predictors.

```{r, warning=FALSE}
R2_mod2b <- partR2(mod2, partvars = c("Temperature", "Precipitation", "Precipitation:Temperature"),
                   data = biomass, max_level = 1)
R2_mod2b
```

Using a Venn diagram, we can show what we actually just estimated. Here, Y is the total variance of the response, Y_RE is the variance explained by random effects, Y_R is the residual variance. The other fields are the variance explaned by the main effects, the interaction and what's shared between them. 

Basically, we estimated part R^2^ for the main effects and the interaction alone, but the
variance shared by both is neither attributed to the main effect nor the interaction. We are
attributing the variance Yx1 (horizontal stripes) to the first main effect (say Temperature) and Yx2 (vertical stripes) to the second main effect (Precipitation). Finally, Yint (horizontal and vertical stripes) is attributed to the the interaction.

![](venn1.jpeg){ width=80% }

This might not be what we are most interest in. Here's a second approach.

#### 3) Pooling main effect and interaction variance
One way to think about variance explained by main effects and their interactions is to pool the variance explained by a main effect with the variance explained by interactions that the term is involved in. Here, Temperature might be important either as a main effect or in interaction with Precipitation and we might want to estimate the total effect of Temperature. The same might be true for Temperature. This can be done using the partbatch argument, which can be used to calculate part R^2^ for batches of predictors.
```{r, warning=FALSE}
R2_mod2c <- partR2(mod2, partbatch = list(c("Temperature", "Temperature:Precipitation"), 
                                          c("Precipitation", "Temperature:Precipitation")),
                        data = biomass)
R2_mod2c
```

We are getting part R^2^'s for the combined effects of each main effect and its interaction. These can again be visualised using a Venn diagram:

![](venn2.jpeg){ width=80% }

Here, X1 is the (combined) part R^2^ for Temperature and Temperature:Precipitation, and X2 is the (combined) part R^2^ for Precipitation and Temperature:Precipitation. 

#### 4) Estimating main effects and interactions in seperate models and combine (Main effect priority)

A fourth, which we think usually preferable option is to prioritize main effects by assigning the proportion of variance that is explained by a main effect together with the variance jointly explained with its interaction to the main effect. This implies that only the part R^2^ for an interaction is calculated from a first model fit (mod3a and R2_mod3a below). Subsequently, in a second model fit, the variance explained by the main effects is estimated when their interaction is excluded from the model (mod4b and R2_mod4b below).  We have implemented a helper function mergeR2 that allows to merge the two partR2 runs.

```{r, warning=FALSE}

mod3a <- lmer(Biomass ~ Temperature * Precipitation + (1|Population), data = biomass)
R2_mod3a <- partR2(mod3a, partvars = c("Temperature:Precipitation"), data = biomass)

mod3b <- lmer(Biomass ~ Temperature + Precipitation + (1|Population), data = biomass)
R2_mod3b <- partR2(mod3a, partvars = c("Temperature", "Precipitation"), data = biomass,
                   max_level = 1)

R2_merged <- mergeR2(R2_mod3a, R2_mod3b)
R2_merged

```

![](venn3.jpeg){ width=80% }

Here, the part R^2^ of Temperature could be X1 (horizontal stripes), the part R^2^ of Precipitation could be X2 (vertical stripes) and the Temperature:Precipitation is X1:X2 or Yint in the Venn diagram. Here, all part R^2^s are rather low because Precipitation and Temperature are correlated.

#### Note on interactions with factorial predictors
An extreme case are interactions involving factorial predictors. With one covariate, a three-level factor and their interaction, the model has estimates six parameters in the fixed part: one intercept, one for the main effect of the covariate, two contrasts for the main-effect of the three-level factor and two contrasts for the interaction. Now, if the covariate is removed from the model, the contrasts will be reparameterized, such that there are still six parameters estimated in the fixed part: one intercept, two contrast for the main-effect of the three-level factor, and three for the interactions.

to be continued ...

### part R^2^s for models with transformations in the formula

We generally advice to do all variable transformation before fitting the model. However, if for some reason this is not possible, it is important to specify the variable in the `partvars` argument exactly (!) how it was done in the model. Here is an example where we fit and additonal term for the precipitation squared in the formula. 

```{r, warning=FALSE}
mod3 <- lmer(Biomass ~ Temperature + Precipitation + I(Precipitation^2) + (1|Population), 
             data = biomass)
```

Now we can specify the exact same term in the `partvars` argument.

```{r, warning=FALSE}
R2_mod3 <- partR2(mod3, partvars = c("Temperature", "Precipitation", "I(Precipitation^2)"), 
                  data = biomass)
R2_mod3
```

### Fitting batches for models with many variables

Sometimes models contain many variables and one is only interested in the combined
effects of some of them. This could for example be the case when dummy coding is
used. In this case, it is possible to calculate part R^2^ in batches using
partbatch or a combination of partvars and partbatch. 

```{r, warning=FALSE}
mod4 <- lmer(Biomass ~ Temperature + Precipitation + SpeciesDiversity + Year + (1|Population), 
             data = biomass)
```

Let's say now we are always ever interested in the combined effects of 
Temperature and Precipitation. `partbatch` takes a list where every element
is a character vector containing variables which should be held together as
batches. 
```{r, warning=FALSE}
R2_mod4 <- partR2(mod4, partvars = "SpeciesDiversity", 
                  partbatch = list(batch1 = "Temperature", "Precipitation"),
                  data = biomass)
R2_mod4
```
If the elements in partbatch have a name, the batch name is used in the output
too.

## Run partR2 in parallel

Parametric bootstrapping is an inherently slow process, because it involves repeated fitting of mixed effects models. Furthermore, the computation time increases exponentially with the number of terms requested, all being tested in isolation and in combination. It is therefore advisable to run preliminary analysis first with low numbers of bootstraps, just to see that things work and make sense in principle. The final analysis should be done with a large number of bootstraps (at least 100, better 1000). This can take time.

A simple way to save runtime is to distribute the bootstrap iterations across multiple cores. `partR2` parallises with the `future` (Bengtsson, 2019) and `furrr` (Vaughan & Dancho, 2018) packages. This makes it flexible for the user on how exactly to parallelise and should work on whatever OS you are running your analyses, be it Mac, Windows or Linux. 

We will illustrate this with the `mod2` fitted above. First we specify how we want to parallelise using `future`'s `plan()` function. Check out `?future::plan` for more information on this. Generally, if you are running your analyses in RStudio, they recommend using `plan(multisession)`. After specifying the plan, you only need to supply a `parallel = TRUE` argument to `partR2` and everything will run in parallel. If you didn't specify a plan, `partR2` will just run sequencially.

```{r, warning=FALSE, message=FALSE}
library(future)
# how many cores do I have?
parallel::detectCores()
# specify plan
plan(multisession, workers = 3)
R2_mod2 <- partR2(mod2, partvars = c("Temperature", "Precipitation"), 
                        nboot = 10, parallel = TRUE, data = biomass)
```

## Partitioning R^2^ in Poisson and binomial models

Partitioning the R2 in Poisson and binomial models works in exactly the same way and with the same function.  We simulated a (Poisson distributed) respone variable `Extinction` for the biomass dataset.

```{r, fig.width=6, fig.height=4}
hist(biomass$Extinction)
```

Let's first fit a Poisson model using `glmer`.

```{r, warning=FALSE}
mod4 <- glmer(Extinction ~ Year +  Temperature * Precipitation + SpeciesDiversity + (1|Population), 
            data=biomass, family="poisson")
```

Then we can use partR2 as before. 

```{r, warning = FALSE}
R2_mod4 <- partR2(mod4, partvars = c("Temperature", "Precipitation"), 
                   R2_type = "marginal", nboot = NULL, CI = 0.95, data=biomass)
print(R2_mod4, round_to = 3) # rounding decimals in print and summary
```
One thing to have in mind when using GLMMs are observation-level random effects (OLRE, i.e. Harrison 2014). `partR2` fits them internally to quantify overdispersion but will recognise an OLRE when it is already in the model. We do not fit an OLRE for binomial models with binary responses because in this case there is no overdispersion. 

Finally, a simple example for a binomial model with a proportion response.

```{r, message=FALSE, warning=FALSE}
mod5 <- glmer(cbind(Recovered, NotRecovered) ~ Year +  
              Temperature * Precipitation + SpeciesDiversity + (1|Population), 
              data=biomass, family="binomial")
```

And we just pass it to `partR2()` again. The internally fitted OLRE is called overdisp in the summary output. 

```{r, message=FALSE, warning=FALSE}
R2_mod5 <- partR2(mod5, partvars = c("Temperature", "Precipitation"), 
                   R2_type = "marginal", nboot = NULL, CI = 0.95, data=biomass)
summary(R2_mod5, round_to=3)
```

As you might have noticed, both Poisson and binomials models give low R^2^ and R values -- even though they have been generated with similar paraameter settings as the Gaussian data. This situation is common and originates from the inherent rounding when traits are expressed as discrete or binary numbers. Fixed and random effects explain variation only at the latent scale, but inherent distribution-specific variance contribute to the residual variances. This leads to large residual variance and relatively lower variance components.

## rptR and partR2: ratios of variance components

The `rptR` (Stoffel, Nakagawa, Schielzeth 2017) package and the `partR2` package go hand in hand. The intra-class coefficient or repeatability calculated in `rptR` is the proportion of variance explained by random effects while the `partR2` package estimates the proportion of variance explained by fixed effects (or fixed plus random effects when `R2_type = "conditional"`). Both the repeatability R and the coefficient of determination R^2^ are therefore ratios of variance components. However, we now changed the design of the `partR2` package compared to `rptR`. The `partR2` user now fits the model first in `lme4` so that any modeling problem can be recognized straight away. We think this is generally preferable as it seperates the modeling part from the rest of the calculations and allows the user to focus on specifying an appropriate model first. 


## Literature

Nakagawa, S. & Schielzeth, H. (2013). A general and simple method for obtaining R2 from generalized linear mixed‐effects models. Methods in ecology and evolution, 4(2), 133-142.

Bates, D. M., Maechler, M., Bolker, B. M. & Walker, S. (2015). Fitting linear mixed‐effects models using lme4. Journal of Statistical Software 67 (1): 1–48.

Wickham, H. (2016). ggplot2: elegant graphics for data analysis. Springer, London.

Schielzeth, H. (2010). Simple means to improve the interpretability of regression coefficients. Methods in Ecology and Evolution, 1(2), 103-113.

Bengtsson, H. (2019). future: Unified Parallel and Distributed Processing in R for Everyone. R
package version 1.14.0. https://CRAN.R-project.org/package=future

Vaughan, D. & Dancho, M. (2018). furrr: Apply Mapping Functions in Parallel using Futures. R
package version 0.1.0. https://CRAN.R-project.org/package=furrr

Harrison, X. A. (2014). Using observation-level random effects to model overdispersion in count data in ecology and evolution. PeerJ, 2, e616.

Stoffel, M. A., Nakagawa, S. & Schielzeth, H. (2017). rptR: Repeatability estimation and variance decomposition by generalized linear mixed‐effects models. Methods in Ecology and Evolution, 8(11), 1639-1644.

Yeatts, P.E., Barton, M., Henson, R.K. & Martin, S.B. (2017). The use of structure coefficients to address multicollinearity in sport and exercise science. Measurement in Physical Education and Exercise Science, 21(2), pp.83-91.

